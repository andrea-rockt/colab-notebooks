{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project-nessie-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeqh5B43Vx9Y0cUGHrhUSs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrea-rockt/colab-notebooks/blob/main/project_nessie_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nessie lab\n",
        "\n",
        "We need to build our own enviroment to test things out and learn about git like file systems\n",
        "\n",
        "* as data engineers we want to try things out in order to properly understand systems that we are building.\n",
        "\n",
        "* as data scientist we want an environment able to support our experimentations.\n",
        "\n",
        "* as developers we want reproducible enviroments to validate our code on.\n",
        "\n",
        "Let's begin by configuring our environment, nobody has ever been fired by defining a bit of infrastructure.\n",
        "\n",
        "We are going to create an environment based on\n",
        "\n",
        "* Apache Spark: our distributed execution engine, this will be the compute layer of our lab environment and will shuffle data around your cluster and crunch the numbers.\n",
        "* The local filesystem: we need to store the actual data on a distributed filesystem, we are only going to only use one node so we will select the local filesystem viewing it as a *special* case of a more general distributed filesystem.\n",
        "* Project nessie: our metadata management solution, table formats describe plain files as collection of related content by attaching metadata to those files, we will store this metadata inside project nessie in order to get time travel on metadata.  \n",
        "\n",
        "# Installing prerequisites\n",
        "\n",
        "We are going to configure this colab instance by:\n",
        "\n",
        "* downloading `spark-3.1.2`\n",
        "* downloading a binary distribution of `nessie`\n",
        "* downloading `ngrok`\n",
        "\n",
        "We are going to access web uis via tunnels provided by `ngrok` (register with your github account or google account on `ngrok.com` and get your auth token)\n",
        "\n",
        "replace `THE_AUTH_TOKEN_FOR_NGROK` with your actual auth token"
      ],
      "metadata": {
        "id": "3Io8hXp5nEUB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dnXdnDVTH-V",
        "outputId": "737a514a-dfb9-4663-fa3a-68fe643cfc28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing SPARK\n",
            "Installing FINDSPARK\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 130 kB 10.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 51.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 70.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 70.2 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling NESSIE\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "mkdir -p build\n",
        "cd build\n",
        "echo \"Installing SPARK\"\n",
        "wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "echo \"Installing FINDSPARK\"\n",
        "pip -q install findspark \n",
        "pip -q install pynessie\n",
        "echo \"Installing NESSIE\"\n",
        "wget -q https://github.com/andrea-rockt/colab-notebooks/raw/main/data/nessie-quarkus-0.9.2.tar.gz\n",
        "tar xf nessie-quarkus-0.9.2.tar.gz\n",
        "chmod +x nessie-quarkus-0.9.2.bin\n",
        "wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
        "tar xf ngrok-stable-linux-amd64.tgz\n",
        "\n",
        "./ngrok authtoken THE_AUTH_TOKEN_FOR_NGROK"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start nessie as a background process, nessie will serve its web UI at localhost:19120\n",
        "\n",
        "nessie will use in-memory persistence so everything we do will be ephemeral"
      ],
      "metadata": {
        "id": "C2Eze39Js3BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system(\"/content/build/nessie-quarkus-0.9.2.bin -Xmx512m 2>&1 > nessie.log &\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiRWowJcXI4N",
        "outputId": "b7be2f79-dc8e-42f6-b077-ee0f1de4a8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_RCwFjJ7T2UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Firing up spark\n",
        "\n",
        "Let's create a pyspark session."
      ],
      "metadata": {
        "id": "lcS0YU5XltWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/build/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "# Full url of the Nessie API endpoint to nessie\n",
        "url = \"http://localhost:19120/api/v1\"\n",
        "# Where to store nessie tables\n",
        "full_path_to_warehouse = '/warehouse/'\n",
        "# The ref or context that nessie will operate on (if different from default branch).\n",
        "# Can be the name of a Nessie branch or tag or a Nessie commit SHA.\n",
        "ref = \"main\"\n",
        "# Nessie authentication type (BASIC, NONE or AWS)\n",
        "auth_type = \"NONE\"\n",
        "\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"spark-nessie-training\") \\\n",
        "       .config(\"spark.jars.packages\",\n",
        "              \"org.apache.iceberg:iceberg-spark3-runtime:0.12.0,org.projectnessie:nessie-spark-extensions:0.18.0\") \\\n",
        "        .config(\"spark.sql.extensions\", \n",
        "               \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\") \\\n",
        "        .config(\"spark.sql.catalog.nessie.uri\", url) \\\n",
        "        .config(\"spark.sql.catalog.nessie.ref\", ref) \\\n",
        "        .config(\"spark.sql.catalog.nessie.authentication.type\", auth_type) \\\n",
        "        .config(\"spark.sql.catalog.nessie.catalog-impl\", \n",
        "              \"org.apache.iceberg.nessie.NessieCatalog\") \\\n",
        "        .config(\"spark.sql.catalog.nessie.warehouse\", full_path_to_warehouse) \\\n",
        "        .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "        .config(\"spark.sql.catalog.nessie.cache-enabled\",\"false\") \\\n",
        "       .getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "id": "GL8UdHzgpqC2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "3f215792-06b4-407e-f9b3-445535072fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f673c3d91f89:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>spark-nessie-training</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f6dad434390>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input dataset preparation\n",
        "\n",
        "We are going to prepare data directly in the main branch to simulate a starting state of our initial data pipeline"
      ],
      "metadata": {
        "id": "W9rtLVdA-01y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we are going to download a dataset of nba players\n",
        "!wget -q https://github.com/sivabalanb/Data-Analysis-with-Pandas-and-Python/raw/master/nba.csv"
      ],
      "metadata": {
        "id": "uDTF-MF_cx6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, DecimalType\n",
        "from pyspark.sql.functions import mean\n",
        "playersSchema = StructType([\n",
        "  StructField(\"Name\",StringType(),False), \\\n",
        "  StructField(\"Team\",StringType(),True), \\\n",
        "  StructField(\"Number\",StringType(),True), \\\n",
        "  StructField(\"Position\", StringType(), True), \\\n",
        "  StructField(\"Age\", StringType(), True), \\\n",
        "  StructField(\"Height\", StringType(), True), \\\n",
        "  StructField(\"Weight\", DoubleType(), True), \\\n",
        "  StructField(\"College\", StringType(), True), \\\n",
        "  StructField(\"Salary\", DecimalType(14, 2), True)\n",
        "])\n"
      ],
      "metadata": {
        "id": "xUyF5eXT_cAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "playersDfRaw = spark.read.csv('nba.csv', header=True, schema=playersSchema)\n",
        "playersDf = playersDfRaw.select(playersDfRaw.Name,\n",
        "                                playersDfRaw.Team,\n",
        "                                playersDfRaw.Number.cast(IntegerType()),\n",
        "                                playersDfRaw.Position,\n",
        "                                playersDfRaw.Age.cast(IntegerType()),\n",
        "                                playersDfRaw.Height,\n",
        "                                playersDfRaw.Weight,\n",
        "                                playersDfRaw.College,\n",
        "                                playersDfRaw.Salary)\n",
        "createPlayersTableStatement = \"\"\"\n",
        "CREATE TABLE if not exists nessie.nba.player (\n",
        "  Name STRING,\n",
        "  Team STRING,\n",
        "  Number INTEGER,\n",
        "  Position STRING,\n",
        "  Age INTEGER,\n",
        "  Height STRING,\n",
        "  Weight DOUBLE,\n",
        "  College STRING,\n",
        "  Salary DECIMAL(14,2)\n",
        ") USING iceberg\n",
        "\"\"\"\n",
        "\n",
        "createSalaryTableStatement = \"\"\"\n",
        "CREATE TABLE if not exists nessie.nba.salary (\n",
        "  Position STRING,\n",
        "  MeanSalary DECIMAL(14,2)\n",
        ") USING iceberg\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(createPlayersTableStatement)\n",
        "spark.sql(createSalaryTableStatement)\n",
        "\n",
        "playersDf.write.format('iceberg').mode('overwrite').save('nessie.nba.player')\n",
        "playersDf.groupBy('Position').agg(mean('Salary').alias('MeanSalary')).write.format('iceberg').mode('overwrite').save('nessie.nba.salary')"
      ],
      "metadata": {
        "id": "low2oCdCA5jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SHOW LOG fix_null_salaries IN nessie').selectExpr('author', 'message','hash').show()\n",
        "spark.sql('CREATE TAG initial_state IN nessie')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsQdBjvPGWZ5",
        "outputId": "d89b52da-f7a4-414e-c3e1-40e8552aec6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+--------------------+\n",
            "|author|       message|                hash|\n",
            "+------+--------------+--------------------+\n",
            "|  root|iceberg commit|5404b96ae155513a7...|\n",
            "|  root|iceberg commit|235c36f81f563bf25...|\n",
            "|  root|iceberg commit|2bab29394448aa07a...|\n",
            "|  root|iceberg commit|ae530ac6021d7a974...|\n",
            "+------+--------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[refType: string, name: string, hash: string]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('USE REFERENCE initial_state IN nessie')\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "  SUM(CAST (Salary   is NULL as INTEGER)) ,\n",
        "  SUM(CAST (College  is NULL as INTEGER)) ,\n",
        "  SUM(CAST (Weight   is NULL as INTEGER)) ,\n",
        "  SUM(CAST (Height   is NULL as INTEGER)) ,\n",
        "  SUM(CAST (Age      is NULL as INTEGER)) ,\n",
        "  SUM(CAST (Position is NULL as INTEGER)) ,\n",
        "  SUM(CAST (Number   is NULL as INTEGER)) ,\n",
        "  SUM(CAST (Team     is NULL as INTEGER)) ,\n",
        "  SUM(CAST (Name     is NULL as INTEGER))\n",
        "FROM \n",
        "  nessie.nba.player \n",
        "\"\"\").show()\n",
        "\n",
        "\n",
        "spark.sql('CREATE BRANCH fix_null_row IN nessie FROM main')\n",
        "spark.sql('CREATE BRANCH fix_null_salaries IN nessie FROM main')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL0f05ZZrZ5h",
        "outputId": "fae6c65a-e6f5-481d-e550-ffa34533c4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+-----------------------------------+----------------------------------+----------------------------------+-------------------------------+------------------------------------+----------------------------------+--------------------------------+--------------------------------+\n",
            "|sum(CAST((Salary IS NULL) AS INT))|sum(CAST((College IS NULL) AS INT))|sum(CAST((Weight IS NULL) AS INT))|sum(CAST((Height IS NULL) AS INT))|sum(CAST((Age IS NULL) AS INT))|sum(CAST((Position IS NULL) AS INT))|sum(CAST((Number IS NULL) AS INT))|sum(CAST((Team IS NULL) AS INT))|sum(CAST((Name IS NULL) AS INT))|\n",
            "+----------------------------------+-----------------------------------+----------------------------------+----------------------------------+-------------------------------+------------------------------------+----------------------------------+--------------------------------+--------------------------------+\n",
            "|                                12|                                 85|                                 1|                                 1|                              1|                                   1|                                 1|                               1|                               1|\n",
            "+----------------------------------+-----------------------------------+----------------------------------+----------------------------------+-------------------------------+------------------------------------+----------------------------------+--------------------------------+--------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[refType: string, name: string, hash: string]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8IxuS_m3rX6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('USE REFERENCE fix_null_row IN nessie')\n",
        "\n",
        "\n",
        "spark.sql(\n",
        "\"\"\"\n",
        "DELETE FROM nessie.nba.player\n",
        "WHERE\n",
        "Salary is NULL AND\n",
        "College is NULL AND\n",
        "Weight is NULL AND\n",
        "Height is NULL AND\n",
        "Age is NULL AND\n",
        "Position is NULL AND\n",
        "Number is NULL AND\n",
        "Team is NULL AND\n",
        "Name is NULL \n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "x_sJ7E0m6O6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067faaee-7d5f-43b7-da99-69f418b6d4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('USE REFERENCE fix_null_salaries IN nessie')\n",
        "\n",
        "spark.read.format('iceberg').load('nessie.nba.player').where('salary is NULL').show()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "UPDATE nessie.nba.player\n",
        "SET Salary = 100000.00\n",
        "WHERE Salary is NULL\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "spark.sql('select * from nessie.nba.player').groupBy('Position').agg(mean('Salary').alias('MeanSalary')).write.format('iceberg').mode('overwrite').save('nessie.nba.salary')\n",
        "spark.sql('select * from nessie.nba.player').groupBy('Position').agg(mean('Salary').alias('MeanSalary')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOhqBKhBJkqp",
        "outputId": "595b67bc-6bf9-43fe-a2da-4e8a8657ecac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+------+--------+----+------+------+--------------------+------+\n",
            "|          Name|                Team|Number|Position| Age|Height|Weight|             College|Salary|\n",
            "+--------------+--------------------+------+--------+----+------+------+--------------------+------+\n",
            "|  John Holland|      Boston Celtics|    30|      SG|  27|   6-5| 205.0|   Boston University|  null|\n",
            "|   Elton Brand|  Philadelphia 76ers|    42|      PF|  37|   6-9| 254.0|                Duke|  null|\n",
            "| Dahntay Jones| Cleveland Cavaliers|    30|      SG|  35|   6-6| 225.0|                Duke|  null|\n",
            "| Jordan Farmar|   Memphis Grizzlies|     4|      PG|  29|   6-2| 180.0|                UCLA|  null|\n",
            "|  Ray McCallum|   Memphis Grizzlies|     5|      PG|  24|   6-3| 190.0|             Detroit|  null|\n",
            "|Xavier Munford|   Memphis Grizzlies|    14|      PG|  24|   6-3| 180.0|        Rhode Island|  null|\n",
            "|Alex Stepheson|   Memphis Grizzlies|    35|      PF|  28|  6-10| 270.0|                 USC|  null|\n",
            "| Briante Weber|          Miami Heat|    12|      PG|  23|   6-2| 165.0|Virginia Commonwe...|  null|\n",
            "| Dorell Wright|          Miami Heat|    11|      SF|  30|   6-9| 205.0|                null|  null|\n",
            "|  Axel Toupane|      Denver Nuggets|     6|      SG|  23|   6-7| 210.0|                null|  null|\n",
            "|    Greg Smith|Minnesota Timberw...|     4|      PF|  25|  6-10| 250.0|        Fresno State|  null|\n",
            "|          null|                null|  null|    null|null|  null|  null|                null|  null|\n",
            "+--------------+--------------------+------+--------+----+------+------+--------------------+------+\n",
            "\n",
            "+--------+--------------+\n",
            "|Position|    MeanSalary|\n",
            "+--------+--------------+\n",
            "|      PG|4861401.858696|\n",
            "|      SF|4801423.247059|\n",
            "|      SG|3894865.274510|\n",
            "|      PF|4428608.500000|\n",
            "|       C|5967052.000000|\n",
            "|    null| 100000.000000|\n",
            "+--------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SHOW LOG IN nessie').show()\n",
        "spark.sql('SHOW LOG main IN nessie').selectExpr('1 as main', '*').join(\n",
        "spark.sql('SHOW LOG fix_null_row IN nessie').selectExpr('0 as main', '*'), 'hash', 'fullouter').sort('hash').show(100000)\n"
      ],
      "metadata": {
        "id": "niUWVS_jdsA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b781888-92d0-40bb-9cae-69863b7f33de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+--------------------+--------------+-----------+--------------------+--------------------+--------------------+\n",
            "|author|committer|                hash|       message|signedOffBy|          authorTime|       committerTime|          properties|\n",
            "+------+---------+--------------------+--------------+-----------+--------------------+--------------------+--------------------+\n",
            "|  root|         |a1da59817c66cbf8e...|iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|  root|         |e490defc27af52a10...|iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|  root|         |5404b96ae155513a7...|iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|  root|         |235c36f81f563bf25...|iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|  root|         |2bab29394448aa07a...|iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|  root|         |ae530ac6021d7a974...|iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "+------+---------+--------------------+--------------+-----------+--------------------+--------------------+--------------------+\n",
            "\n",
            "+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+\n",
            "|                hash|main|author|committer|       message|signedOffBy|          authorTime|       committerTime|          properties|main|author|committer|       message|signedOffBy|          authorTime|       committerTime|          properties|\n",
            "+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+\n",
            "|235c36f81f563bf25...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|2bab29394448aa07a...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|5404b96ae155513a7...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|ae530ac6021d7a974...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|e4ca7e4ca8f360569...|null|  null|     null|          null|       null|                null|                null|                null|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('MERGE BRANCH fix_null_row INTO main IN nessie').collect()\n",
        "spark.sql('MERGE BRANCH fix_null_salaries INTO main IN nessie').collect()\n",
        "spark.sql('SHOW LOG main IN nessie').selectExpr('1 as main', '*').join(\n",
        "spark.sql('SHOW LOG fix_null_salaries IN nessie').selectExpr('0 as main', '*'), 'hash', 'fullouter').show(100000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4esKTK6o3jo",
        "outputId": "6dbd3688-8ede-47a2-bda1-0d2586054819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+\n",
            "|                hash|main|author|committer|       message|signedOffBy|          authorTime|       committerTime|          properties|main|author|committer|       message|signedOffBy|          authorTime|       committerTime|          properties|\n",
            "+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+\n",
            "|235c36f81f563bf25...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|c8a5e53946de0f055...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|null|  null|     null|          null|       null|                null|                null|                null|\n",
            "|5404b96ae155513a7...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|a1da59817c66cbf8e...|null|  null|     null|          null|       null|                null|                null|                null|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|2bab29394448aa07a...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|e4ca7e4ca8f360569...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|null|  null|     null|          null|       null|                null|                null|                null|\n",
            "|ae530ac6021d7a974...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "|9b1a0f94faeedbaef...|   1|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|null|  null|     null|          null|       null|                null|                null|                null|\n",
            "|e490defc27af52a10...|null|  null|     null|          null|       null|                null|                null|                null|   0|  root|         |iceberg commit|           |2022-01-20 12:05:...|2022-01-20 12:05:...|{application-type...|\n",
            "+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+----+------+---------+--------------+-----------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}